// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
  if(type == 'memory'){
    try {
      if(obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
        return params.max_memory as nextflow.util.MemoryUnit
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
      return obj
    }
  } else if(type == 'time'){
    try {
      if(obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
        return params.max_time as nextflow.util.Duration
      else
        return obj
    } catch (all) {
      println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
      return obj
    }
  } else if(type == 'cpus'){
    try {
      return Math.min( obj, params.max_cpus as int )
    } catch (all) {
      println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
      return obj
    }
  }
}

py3Image = "quay.io/h3abionet_org/py3plink"
gemmaImage="quay.io/h3abionet_org/py3plink"
latexImage="quay.io/h3abionet_org/h3agwas-texlive"
py2fastImage="quay.io/h3abionet_org/py2fastlmm"

swarmPort = '2376'
queue = 'batch'

manifest {
    homePage = 'http://github.com/h3abionet/h3agwas'
    description = 'GWAS Pipeline for H3Africa'
    mainScript = "main.nf"
}


cloud {
    // imageId = "ami-710b9108"      // specify your AMI id here
    instanceType = "m4.xlarge"
    subnetId = "null"
    sharedStorageId   = "null"
    sharedStorageMount = "/mnt/shared"
    bootStorageSize = "20GB"     // Size of disk for images spawned
    //   instanceStorageMount = ""   // Set a common mount point for images
    //   instanceStorageDevice = ""  // Set a common block device for images
    autoscale {
        enabled = true
        maxInstances = 1
        terminateWhenIdle = true
    }

}


params {

    // Directories
    work_dir                = "/$PWD"
    input_dir               = "sample"
    // Can use S3 too
    //input_dir               = "s3://h3abionet/sample"
    input_pat               = "sampleA"

    output_dir              = "results"
    scripts                 = "$baseDir/scripts"
    output                  = "out"

    max_forks            = 95     

    high_ld_regions_fname = ""
    sexinfo_available    = true
    cut_het_high         = 0.343
    cut_het_low           = 0.15
    cut_diff_miss         = "0.05"
    cut_maf                = "0.01"
    cut_mind              = "0.02"
    cut_geno              = 0.01
    cut_hwe               = 0.008
    pi_hat                = 0.11
    super_pi_hat 	  = 0.7
    f_lo_male             = 0.8 // default for F-sex check -- >= means male
    f_hi_female           = 0.2 //  <= means female
    gc10                 = 0.4  // 10% Gen Call confidence -- 0.4 is very low
    case_control         = "${params.input_dir}/sample.phe"
    case_control_col     = "PHE"


    phenotype = "0"
    pheno_col = ""
    batch = "0"
    batch_col = 0

    idpat                =  0  // or "(\\w+)-DNA_(\\w+)_.*" or ".*_(.*)"

    plink_mem_req        = "1750MB"
    other_mem_req        = "1750MB"
    big_time             = '12h'
    sharedStorageMount   = "/mnt/shared"
    max_plink_cores      = 4


}

docker.enabled = true
docker.runOptions = "-u \$(id -u):\$(id -g)"
profiles {


    // For execution on a local machine, no containerization. -- Default
    standard {
       
    }

    test {
        includeConfig "conf/test.config"
    }

    awsbatch {
         process.executor = "awsbatch"
	 aws.region    ='us-east-1'
         aws.uploadStorageClass = 'ONEZONE_IA'
         process.queue = 'h3a-00'
    }


    slurm {
       process.executor = 'slurm'
       process.queue = queue
     }



    // Execute pipeline with Docker locally
    docker {
        process.executor = 'local'
        docker.remove      = true
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
	docker.runOptions = '-u $(id -u):$(id -g) --rm'
        docker.process.executor = 'local'
    }


    // Execute pipeline with Docker Swarm setup
    dockerSwarm {
        docker.remove      = true
        docker.runOptions  = '--rm'
        docker.registry    = 'quay.io'
        docker.enabled     = true
        docker.temp        = 'auto'
        docker.fixOwnership= true
        docker.process.executor = 'local'
        docker.engineOptions = "-H :$swarmPort"
    }

    // For execution on a PBS scheduler, no containerization.
    pbs {
        process.executor = 'pbs'
        process.queue = queue
    }




    // For execution on a SLURM scheduler, no containerization.
    slurm {
        process.executor = 'slurm'
        process.queue = queue
    }







    singularity {
        singularity.cacheDir = "${HOME}/.singularity"
        singularity.autoMounts = true
        singularity.enabled = true
        process.executor = 'local'
     }
}






process {

    withLabel:bigMem {
      memory = '8GB'
    }

    container=py3Image

    withLabel: latex {
          container = latexImage
    }

    withLabel: py2fast {
          container = py2fastImage 
    }

}

timeline { 
    enabled=true
    file = "nextflow_reports/timeline.html"
}

report {
    enabled = true
    file = "nextflow_reports/report.html"
}
